{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aMCTS_parallel import MCTSParallel, MCTS_Node\n",
    "from ataxx import AttaxxBoard\n",
    "from fastgo import GoBoard\n",
    "\n",
    "import random\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the data augmentation process\n",
    "def transformations(board_state, action_probs, outcome, gameType, fill_size=0):\n",
    "    if gameType == 'G':\n",
    "        side = board_state.size\n",
    "        transf = []\n",
    "        # Flip vertically    \n",
    "        transf.append((board_state.flip_vertical().EncodedGameStateChanged(), np.append(np.flip(np.copy(action_probs)[:-1].reshape(side,side),0).flatten(),action_probs[-1]), outcome))                         # flip vertically\n",
    "        # Rotate 90 degrees\n",
    "        transf.append((board_state.rotate90(1).EncodedGameStateChanged(), np.append(np.rot90(np.copy(action_probs)[:-1].reshape(side,side),1).flatten(),action_probs[-1]), outcome))                            # rotate 90\n",
    "        # Rotate 90 degrees and flip vertically\n",
    "        transf.append((board_state.rotate90(1).flip_vertical().EncodedGameStateChanged(), np.append(np.rot90(np.flip(np.copy(action_probs)[:-1].reshape(side,side),1),0).flatten(),action_probs[-1]), outcome)) # rotate 90 and flip vertically\n",
    "        # Rotate 180 degrees\n",
    "        transf.append((board_state.rotate90(2).EncodedGameStateChanged(), np.append(np.rot90(np.copy(action_probs)[:-1].reshape(side,side),2).flatten(),action_probs[-1]), outcome))                            # rotate 180\n",
    "        # Rotate 180 degrees and flip vertically\n",
    "        transf.append((board_state.rotate90(2).flip_vertical().EncodedGameStateChanged(), np.append(np.rot90(np.flip(np.copy(action_probs)[:-1].reshape(side,side),1),0).flatten(),action_probs[-1]), outcome)) # rotate 180 and flip vertically\n",
    "        # Rotate 270 degrees\n",
    "        transf.append((board_state.rotate90(3).EncodedGameStateChanged(), np.append(np.rot90(np.copy(action_probs)[:-1].reshape(side,side),3).flatten(),action_probs[-1]), outcome))                            # rotate 270\n",
    "        # Rotate 270 degrees and flip vertically\n",
    "        transf.append((board_state.rotate90(3).flip_vertical().EncodedGameStateChanged(), np.append(np.rot90(np.flip(np.copy(action_probs)[:-1].reshape(side,side),1),0).flatten(),action_probs[-1]), outcome)) # rotate 270 and flip vertically\n",
    "        return transf\n",
    "    elif gameType == 'A':\n",
    "        if fill_size==0:\n",
    "            side = board_state.size\n",
    "            transf = []\n",
    "            # Flip vertically \n",
    "            transf.append((board_state.flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.copy(action_probs).reshape(side,side,side,side),2),0).flatten(), outcome))                                                 # flip vertically\n",
    "            # Rotate 90 degrees\n",
    "            transf.append((board_state.rotate90(1).EncodedGameStateChanged(), np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),1,(2,3)),1,(0,1)).flatten(), outcome))                                       # rotate 90\n",
    "            # Rotate 90 degrees and flip vertically\n",
    "            transf.append((board_state.rotate90(1).flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),1,(2,3)),1,(0,1)),2),0).flatten(), outcome)) # rotate 90 and flip vertically\n",
    "            # Rotate 180 degrees\n",
    "            transf.append((board_state.rotate90(2).EncodedGameStateChanged(), np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),2,(2,3)),2,(0,1)).flatten(), outcome))                                       # rotate 180\n",
    "            # Rotate 180 degrees and flip vertically\n",
    "            transf.append((board_state.rotate90(2).flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),2,(2,3)),2,(0,1)),2),0).flatten(), outcome)) # rotate 180 and flip vertically\n",
    "            # Rotate 270 degrees\n",
    "            transf.append((board_state.rotate90(3).EncodedGameStateChanged(), np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),3,(2,3)),3,(0,1)).flatten(), outcome))                                       # rotate 270\n",
    "            # Rotate 270 degrees and flip vertically\n",
    "            transf.append((board_state.rotate90(3).flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),3,(2,3)),3,(0,1)),2),0).flatten(), outcome)) # rotate 270 and flip vertically\n",
    "            return transf\n",
    "        else:\n",
    "            side = board_state.size\n",
    "            transf = []\n",
    "            # Flip vertically \n",
    "            transf.append((board_state.flip_vertical().EncodedGameStateChanged(fill_size), np.pad(np.flip(np.flip(np.copy(action_probs).reshape(fill_size,fill_size,fill_size,fill_size)[:side,:side,:side,:side],2),0),(0,fill_size-side),'constant',constant_values=(0)).flatten(), outcome))                                                 # flip vertically\n",
    "            # Rotate 90 degrees\n",
    "            transf.append((board_state.rotate90(1).EncodedGameStateChanged(fill_size),np.pad(np.rot90(np.rot90(np.copy(action_probs).reshape(fill_size,fill_size,fill_size,fill_size)[:side,:side,:side,:side],1,(2,3)),1,(0,1)),(0,fill_size-side),'constant',constant_values=(0)).flatten(), outcome))                                       # rotate 90\n",
    "            # Rotate 90 degrees and flip vertically\n",
    "            transf.append((board_state.rotate90(1).flip_vertical().EncodedGameStateChanged(fill_size), np.pad(np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(fill_size,fill_size,fill_size,fill_size)[:side,:side,:side,:side],1,(2,3)),1,(0,1)),2),0),(0,fill_size-side),'constant',constant_values=(0)).flatten(), outcome)) # rotate 90 and flip vertically\n",
    "            # Rotate 180 degrees\n",
    "            transf.append((board_state.rotate90(2).EncodedGameStateChanged(fill_size), np.pad(np.rot90(np.rot90(np.copy(action_probs).reshape(fill_size,fill_size,fill_size,fill_size)[:side,:side,:side,:side],2,(2,3)),2,(0,1)),(0,fill_size-side),'constant',constant_values=(0)).flatten(), outcome))                                       # rotate 180\n",
    "            # Rotate 180 degrees and flip vertically\n",
    "            transf.append((board_state.rotate90(2).flip_vertical().EncodedGameStateChanged(fill_size), np.pad(np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(fill_size,fill_size,fill_size,fill_size)[:side,:side,:side,:side],2,(2,3)),2,(0,1)),2),0),(0,fill_size-side),'constant',constant_values=(0)).flatten(), outcome)) # rotate 180 and flip vertically\n",
    "            # Rotate 270 degrees\n",
    "            transf.append((board_state.rotate90(3).EncodedGameStateChanged(fill_size), np.pad(np.rot90(np.rot90(np.copy(action_probs).reshape(fill_size,fill_size,fill_size,fill_size)[:side,:side,:side,:side],3,(2,3)),3,(0,1)),(0,fill_size-side),'constant',constant_values=(0)).flatten(), outcome))                                       # rotate 270\n",
    "            # Rotate 270 degrees and flip vertically\n",
    "            transf.append((board_state.rotate90(3).flip_vertical().EncodedGameStateChanged(fill_size), np.pad(np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(fill_size,fill_size,fill_size,fill_size)[:side,:side,:side,:side],3,(2,3)),3,(0,1)),2),0),(0,fill_size-side),'constant',constant_values=(0)).flatten(), outcome)) # rotate 270 and flip vertically\n",
    "            return transf\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Parameter for Exploratory Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that applies temperature to the given probabilities distribution and normalizes the result, for the current AlphaZero iteration\n",
    "def probs_with_temperature(probabilities, az_iteration):\n",
    "    # returns a vale between 1.25 and 0.75\n",
    "    def temperature_function(az_iter):\n",
    "        return 1 / (1 + np.e**(az_iter-5)) + 0.5\n",
    "    prob_temp =  probabilities**(1/temperature_function(az_iteration))\n",
    "    prob_temp /= np.sum(prob_temp)\n",
    "    return prob_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaZero Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel2:\n",
    "    \"\"\"\n",
    "    Class implementing the AlphaZero algorithm with parallelized self-play, MCTS, and training.\n",
    "\n",
    "    Parameters:\n",
    "        - model: The neural network model.\n",
    "        - optimizer: The optimizer used for training the neural network.\n",
    "        - board: The game board.\n",
    "        - gameType: Type of the game ('G' for Go, 'A' for Attaxx).\n",
    "        - data_augmentation: Flag for enabling data augmentation during self-play.\n",
    "        - verbose: Flag for printing progress information.\n",
    "        - fill_size: The fill size (used for Attaxx game with a fill).\n",
    "        - **params: Additional parameters for configuration.\n",
    "\n",
    "    Methods:\n",
    "        - SelfPlay: Perform self-play for a specified number of iterations.\n",
    "        - Train: Train the neural network on a given dataset.\n",
    "        - Learn: Execute the AlphaZero algorithm for a specified number of iterations.\n",
    "\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, model, optimizer, board, gameType, data_augmentation=False, verbose=False, fill_size=0, **params):\n",
    "        \"\"\"\n",
    "        Initialize the AlphaZeroParallel2 object.\n",
    "\n",
    "        Parameters:\n",
    "            - model: The neural network model.\n",
    "            - optimizer: The optimizer used for training the neural network.\n",
    "            - board: The game board.\n",
    "            - gameType: Type of the game ('G' for Go, 'A' for Attaxx).\n",
    "            - data_augmentation: Flag for enabling data augmentation during self-play.\n",
    "            - verbose: Flag for printing progress information.\n",
    "            - fill_size: The fill size (used for Attaxx game with a fill).\n",
    "            - **params: Additional parameters for configuration.\n",
    "\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.board = board\n",
    "        self.gameType = gameType\n",
    "        self.params = params\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.verbose = verbose\n",
    "        self.fill_size = fill_size\n",
    "\n",
    "    def SelfPlay(self, az_iteration):\n",
    "        \"\"\"\n",
    "        Perform self-play for a specified number of iterations.\n",
    "\n",
    "        Parameters:\n",
    "            - az_iteration: The current iteration of the AlphaZero algorithm.\n",
    "\n",
    "        Returns:\n",
    "            - return_dataset: A list of training samples (state, action probabilities, outcome).\n",
    "\n",
    "        \"\"\"\n",
    "        # Set the size of the game board\n",
    "        if self.fill_size != 0:\n",
    "            size = self.fill_size\n",
    "        else:\n",
    "            size = self.board.size\n",
    "        \n",
    "        # Initialize the return dataset to store training samples\n",
    "        return_dataset = []\n",
    "\n",
    "        # Track the number of self-plays performed\n",
    "        selfplays_done = 0\n",
    "\n",
    "        # Create a list of game boards, each associated with a separate thread for parallel self-play\n",
    "        boards = [None for _ in range(self.params[\"n_self_play_parallel\"])]\n",
    "        boards_dataset = [[] for _ in range(self.params[\"n_self_play_parallel\"])]\n",
    "        boards_play_count = [0 for _ in range(self.params[\"n_self_play_parallel\"])]\n",
    "\n",
    "        # Initialize game boards based on the specified game type (Attaxx or Go)\n",
    "        for i in range(self.params[\"n_self_play_parallel\"]):\n",
    "            boards[i] = AttaxxBoard(size) if self.gameType == \"A\" else GoBoard(size)\n",
    "            boards[i].Start(render=False)\n",
    "\n",
    "            # Adjust the size if the fill_size parameter is set\n",
    "            if self.fill_size != 0:\n",
    "                size -= 1\n",
    "\n",
    "        # Reset size for self-play iterations\n",
    "        if self.fill_size != 0:\n",
    "            size = self.fill_size\n",
    "\n",
    "        # Initialize the MCTS object for parallel search\n",
    "        self.mcts = MCTSParallel(self.model, fill_size=self.fill_size)\n",
    "        root_boards = [MCTS_Node(board, fill_size=self.fill_size) for board in boards]\n",
    "\n",
    "        # Main loop for self-play\n",
    "        while len(boards) > 0:\n",
    "            # Use MCTS to get action probabilities for each board\n",
    "            boards_actions_probs = self.mcts.Search(root_boards, self.params[\"mcts_iterations\"])\n",
    "\n",
    "            # Iterate over boards in reverse order to safely remove boards\n",
    "            for i in range(len(boards))[::-1]:\n",
    "                action_probs = boards_actions_probs[i]\n",
    "\n",
    "                # Append the current state, action probabilities, and player to the dataset\n",
    "                boards_dataset[i].append((boards[i].copy(), action_probs, boards[i].player))\n",
    "\n",
    "                # Choose an action based on the probabilities\n",
    "                moves = list(range(len(action_probs)))\n",
    "                action = np.random.choice(moves, p=action_probs)\n",
    "                move = self.mcts.roots[i].children[action].originMove\n",
    "\n",
    "                # Apply the selected move to the board\n",
    "                boards[i].Move(move)\n",
    "                boards[i].NextPlayer()\n",
    "                boards[i].CheckFinish()\n",
    "                boards_play_count[i] += 1\n",
    "\n",
    "                # Update the new root (root is now the played child state)\n",
    "                root_boards[i] = self.mcts.roots[i].children[action]\n",
    "                root_boards[i].parent = None  # It is needed to \"remove\" / \"delete\" the parent state\n",
    "\n",
    "                # Check if the move cap is reached or the game is finished\n",
    "                if boards_play_count[i] >= self.params[\"move_cap\"] and boards[i].winner == 0:\n",
    "                    boards[i].winner = 3\n",
    "\n",
    "                if boards[i].hasFinished():\n",
    "                    # Append the final configuration to the dataset\n",
    "                    boards_dataset[i].append((boards[i].copy(), action_probs, boards[i].player))\n",
    "\n",
    "                    # Switch to the next player and append the state again\n",
    "                    boards[i].NextPlayer()\n",
    "                    boards_dataset[i].append((boards[i].copy(), action_probs, boards[i].player))\n",
    "\n",
    "                    # Process the dataset and add training samples with outcomes\n",
    "                    for board, action_probs, player in boards_dataset[i]:\n",
    "                        if player == boards[i].winner:\n",
    "                            outcome = 1\n",
    "                        elif 3 - player == boards[i].winner:\n",
    "                            outcome = -1\n",
    "                        else:\n",
    "                            outcome = 0\n",
    "\n",
    "                        # Add the training sample to the return dataset\n",
    "                        return_dataset.append((board.EncodedGameStateChanged(self.fill_size), action_probs, outcome))\n",
    "\n",
    "                        # Data augmentation process (rotating and flipping the board)\n",
    "                        if self.data_augmentation:\n",
    "                            for transformed_data in transformations(board, action_probs, outcome, self.gameType, fill_size=self.fill_size):\n",
    "                                return_dataset.append(transformed_data)\n",
    "\n",
    "                    # Dynamic parallel self-play allocation\n",
    "                    if selfplays_done >= self.params[\"self_play_iterations\"] - self.params[\"n_self_play_parallel\"]:\n",
    "                        del boards[i]\n",
    "                        del root_boards[i]\n",
    "                        del boards_play_count[i]\n",
    "                    else:\n",
    "                        # Initialize a new game board for self-play\n",
    "                        boards[i] = AttaxxBoard(size) if self.gameType == \"A\" else GoBoard(size)\n",
    "                        boards[i].Start(render=False)\n",
    "                        root_boards[i] = MCTS_Node(boards[i], fill_size=self.fill_size)\n",
    "                        boards_dataset[i] = []\n",
    "                        boards_play_count[i] = 0\n",
    "\n",
    "                        # Adjust the size if the fill_size parameter is set\n",
    "                        if self.fill_size != 0:\n",
    "                            if (selfplays_done + 1) % (self.fill_size - 3) == 0:\n",
    "                                size = self.fill_size\n",
    "                            else:\n",
    "                                size -= 1\n",
    "\n",
    "                    selfplays_done += 1\n",
    "\n",
    "                    # Print progress message\n",
    "                    if selfplays_done % self.params[\"n_self_play_parallel\"] == 0:\n",
    "                        print(\"\\nSELFPLAY:\", selfplays_done * 100 // self.params[\"self_play_iterations\"], \"%\")\n",
    "\n",
    "        print(\"\\nSELFPLAY: 100 %\")\n",
    "        return return_dataset\n",
    "\n",
    "    \n",
    "    def Train(self, dataset):\n",
    "        \"\"\"\n",
    "        Train the neural network on a given dataset.\n",
    "\n",
    "        Parameters:\n",
    "            - dataset: The dataset for training.\n",
    "\n",
    "        \"\"\"\n",
    "        random.shuffle(dataset)\n",
    "\n",
    "    # Iterate over the dataset in batches\n",
    "        for batch_index in range(0, len(dataset), self.params['batch_size']):\n",
    "            # Extract a batch of samples from the dataset\n",
    "            sample = dataset[batch_index: batch_index + self.params[\"batch_size\"]]\n",
    "            \n",
    "            # Unzip the samples into separate lists for board_encoded, policy_targets, and value_targets\n",
    "            board_encoded, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            # Convert the lists to NumPy arrays\n",
    "            board_encoded, policy_targets, value_targets = np.array(board_encoded), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "            # Convert NumPy arrays to PyTorch tensors and move them to the device (GPU, if available)\n",
    "            board_encoded = torch.tensor(board_encoded, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # Forward pass: Get the model predictions for policy and value\n",
    "            out_policy, out_value = self.model(board_encoded)\n",
    "\n",
    "            # Calculate policy loss using cross-entropy loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "\n",
    "            # Calculate value loss using mean squared error loss\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "\n",
    "            # Combine policy and value losses with a weight factor for policy loss\n",
    "            loss = policy_loss * 0.1 + value_loss\n",
    "\n",
    "            # Zero the gradients, perform backward pass, and update model parameters\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    def Learn(self):\n",
    "        \"\"\"\n",
    "        Execute the AlphaZero algorithm for a specified number of iterations.\n",
    "\n",
    "        \"\"\"\n",
    "        for az_iteration in tqdm(range(self.params[\"n_iterations\"]), desc=\"AlphaZero Algorithm Iterations\", leave=False, unit=\"iter\", ncols=100, colour=\"#fc6a65\"):\n",
    "            # Set the model in evaluation mode during self-play\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Perform self-play to generate a dataset\n",
    "            dataset = self.SelfPlay(az_iteration)\n",
    "            \n",
    "            # Set the model back in training mode for updating parameters\n",
    "            self.model.train()\n",
    "            \n",
    "            # Iterate over the specified number of training epochs\n",
    "            for epoch in tqdm(range(self.params[\"n_epochs\"]), desc=\"Training Model\", leave=False, unit=\"epoch\", ncols=100, colour=\"#9ffc65\"):\n",
    "                # Train the model using the generated dataset\n",
    "                self.Train(dataset)\n",
    "            \n",
    "            # Save the model and optimizer states after each iteration\n",
    "            if self.fill_size == 0:\n",
    "                torch.save(self.model.state_dict(), f\"./Models/{str.upper(self.gameType)}{self.board.size}/{str.upper(self.gameType)}{self.board.size}_{az_iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"./Optimizers/{str.upper(self.gameType)}{self.board.size}/{str.upper(self.gameType)}{self.board.size}_{az_iteration}_opt.pt\")\n",
    "            else:\n",
    "                torch.save(self.model.state_dict(), f\"./Models/{str.upper(self.gameType)}Flex/{str.upper(self.gameType)}Flex_{az_iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"./Optimizers/{str.upper(self.gameType)}Flex/{str.upper(self.gameType)}Flex_{az_iteration}_opt.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
