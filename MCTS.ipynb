{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "from ataxx import AtaxxBoard\n",
    "from fastgo import GoBoard\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Node Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Node:\n",
    "    def __init__(self, board, parent=None, move=None, policy_value=0, fill_size=0) -> None:\n",
    "        # Initialize MCTS_Node with the given parameters\n",
    "        self.board = board\n",
    "        self.w = 0  # Sum of backpropagation\n",
    "        self.n = 0  # Num of visits\n",
    "        self.p = policy_value  # Probability returned from NN\n",
    "\n",
    "        self.originMove = move\n",
    "        self.parent = parent\n",
    "        self.children = {}  # Save all children\n",
    "        self.fill_size = fill_size\n",
    "\n",
    "    def Select(self):\n",
    "        # UCB constant\n",
    "        c = 2\n",
    "        max_ucb = -numpy.inf\n",
    "        best_node = []\n",
    "\n",
    "        # If the node has no children, return itself\n",
    "        if len(self.children) == 0:\n",
    "            return self\n",
    "\n",
    "        # Iterate through children and calculate UCB for each\n",
    "        for action, child in self.children.items():\n",
    "            if child.n == 0:\n",
    "                ucb = child.p * c * (self.n ** (1 / 2)) / (1 + child.n)\n",
    "            else:\n",
    "                ucb = -(child.w / child.n) + child.p * c * (self.n ** (1 / 2)) / (1 + child.n)\n",
    "\n",
    "            # Update max UCB value, as well as the best Node\n",
    "            if ucb > max_ucb:\n",
    "                max_ucb = ucb\n",
    "                best_node = [child]\n",
    "            elif ucb == max_ucb:\n",
    "                best_node.append(child)\n",
    "\n",
    "        # Randomly choose one of the best nodes\n",
    "        return numpy.random.choice(best_node)\n",
    "\n",
    "    def Expansion(self, policy: numpy.ndarray):\n",
    "        # Expand the node by creating child nodes for possible moves\n",
    "        if len(self.children) != 0 or self.board.winner != 0:\n",
    "            return\n",
    "\n",
    "        # Mask the policy with valid moves\n",
    "        masked_normalized_policy = numpy.zeros(shape=policy.shape)\n",
    "        possible_moves = list(self.board.PossibleMoves())\n",
    "\n",
    "        for move in possible_moves:\n",
    "            action = self.board.MoveToAction(move, self.fill_size)\n",
    "            masked_normalized_policy[action] = policy[action]\n",
    "\n",
    "        # Normalize the masked policy\n",
    "        if numpy.sum(masked_normalized_policy) != 0 and not numpy.isnan(numpy.sum(masked_normalized_policy)):\n",
    "            masked_normalized_policy /= numpy.sum(masked_normalized_policy)\n",
    "        else:\n",
    "            # If there is any issue with the probabilities, distribute uniform probabilities\n",
    "            masked_normalized_policy = numpy.ones(len(masked_normalized_policy)) / len(possible_moves)\n",
    "\n",
    "        # Create child nodes for each possible move\n",
    "        for move in possible_moves:\n",
    "            cur_board = self.board.copy()\n",
    "            cur_board.Move(move)\n",
    "            action = cur_board.MoveToAction(move)\n",
    "            cur_board.NextPlayer()\n",
    "            cur_board.CheckFinish()\n",
    "            self.children[action] = MCTS_Node(cur_board, self, move, policy_value=masked_normalized_policy[action],\n",
    "                                              fill_size=self.fill_size)\n",
    "\n",
    "    def BackPropagation(self, value):\n",
    "        # Backpropagate the value up the tree\n",
    "        cur = self\n",
    "        while cur is not None:\n",
    "            cur.w += value\n",
    "            cur.n += 1\n",
    "            value *= -1\n",
    "            cur = cur.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MCTSParallel:\n",
    "    def __init__(self, model, fill_size=0) -> None:\n",
    "        # Initialize MCTSParallel with the given parameters\n",
    "        self.model = model\n",
    "        self.roots = []\n",
    "        self.fill_size = fill_size\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def Search(self, root_boards: list[MCTS_Node], n_iterations, test=False):\n",
    "        # Perform MCTS search for a specified number of iterations\n",
    "        self.roots = root_boards\n",
    "\n",
    "        # Add noise to the roots' policy array\n",
    "        boards_states = [root.board.EncodedGameStateChanged(self.fill_size) for root in self.roots]\n",
    "        boards_states = torch.tensor(boards_states, device=self.model.device)\n",
    "        policy, _ = self.model(boards_states)\n",
    "        policy = policy.cpu().numpy()\n",
    "\n",
    "        # Calculate action space size based on game type and fill size\n",
    "        if self.fill_size == 0:\n",
    "            action_space_size = self.roots[0].board.size ** 4 if type(self.roots[0].board) == AtaxxBoard else \\\n",
    "                (self.roots[0].board.size ** 2) + 1\n",
    "        else:\n",
    "            action_space_size = self.fill_size ** 4\n",
    "\n",
    "        # Expand each root and associate the policy values with the ramifications\n",
    "        for i in range(len(root_boards)):\n",
    "            if not test:\n",
    "                possible_moves = self.roots[i].board.PossibleMoves()\n",
    "                masked_normalized_policy = numpy.zeros(shape=policy[i].shape)\n",
    "\n",
    "                # Mask the policy with valid moves\n",
    "                for move in possible_moves:\n",
    "                    action = self.roots[i].board.MoveToAction(move)\n",
    "                    masked_normalized_policy[action] = policy[i][action]\n",
    "\n",
    "                # Normalize the masked policy\n",
    "                masked_normalized_policy /= numpy.sum(masked_normalized_policy)\n",
    "                noise = numpy.random.normal(0, 0.01, len(masked_normalized_policy))\n",
    "                policy[i] = numpy.abs(masked_normalized_policy + noise)\n",
    "\n",
    "            self.roots[i].Expansion(policy[i])\n",
    "            self.roots[i].n += 1\n",
    "\n",
    "        # Start MCTS iterations\n",
    "        for _ in tqdm(range(n_iterations - min([root.n for root in self.roots]) + 1), desc=\"MCTS Iterations\",\n",
    "                      leave=False, unit=\"iter\", ncols=100, colour=\"#f7fc65\"):\n",
    "            nodes = [self.roots[i].Select() for i in range(len(root_boards))]\n",
    "\n",
    "            # Selection phase\n",
    "            for i in range(len(root_boards)):\n",
    "                while len(nodes[i].children) > 0:\n",
    "                    nodes[i] = nodes[i].Select()\n",
    "\n",
    "            # Get values from NN\n",
    "            boards_states = [nodes[i].board.EncodedGameStateChanged(self.fill_size) for i in range(len(root_boards))]\n",
    "            boards_states = torch.tensor(boards_states, device=self.model.device)\n",
    "            policy, value = self.model(boards_states)\n",
    "            policy = policy.cpu().numpy()\n",
    "            value = value.cpu().numpy()\n",
    "\n",
    "            for i in range(len(root_boards)):\n",
    "                if nodes[i].board.winner != 0:\n",
    "                    # If the game has already finished, update the backpropagation value\n",
    "                    if nodes[i].board.winner == nodes[i].board.player:\n",
    "                        nodes[i].BackPropagation(1)\n",
    "                    elif nodes[i].board.winner == 3 - nodes[i].board.player:\n",
    "                        nodes[i].BackPropagation(-1)\n",
    "                    else:\n",
    "                        nodes[i].BackPropagation(0)\n",
    "                else:\n",
    "                    # Expansion phase\n",
    "                    nodes[i].Expansion(policy[i])\n",
    "                    # Backpropagation phase\n",
    "                    nodes[i].BackPropagation(value[i][0].item())\n",
    "\n",
    "        # Return the actions' probabilities for each game\n",
    "        if self.fill_size == 0:\n",
    "            action_space_size = self.roots[0].board.size ** 4 if type(self.roots[0].board) == AtaxxBoard else \\\n",
    "                (self.roots[0].board.size ** 2) + 1\n",
    "        else:\n",
    "            action_space_size = self.fill_size ** 4\n",
    "\n",
    "        boards_actions_probs = [numpy.zeros(shape=action_space_size) for _ in range(len(root_boards))]\n",
    "        for i in range(len(root_boards)):\n",
    "            for action, child in self.roots[i].children.items():\n",
    "                boards_actions_probs[i][action] = child.n\n",
    "            boards_actions_probs[i] /= numpy.sum(boards_actions_probs[i])\n",
    "\n",
    "        return boards_actions_probs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
