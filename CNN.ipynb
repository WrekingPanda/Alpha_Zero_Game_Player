{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Sequential, Conv2d, Module, Softmax, BatchNorm2d\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResBlock Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden, size):\n",
    "        super().__init__()\n",
    "        # Residual block: Convolution + Batch Normalization + ReLU + Convolution + Batch Normalization\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=size, padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=size, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the residual block\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, size, action_size, num_resBlocks, num_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check if a GPU is available, otherwise use CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Initial block: Convolution + Batch Normalization + ReLU\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=size, padding='same'),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Backbone with multiple residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden, size) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        # Policy head: Convolution + Batch Normalization + ReLU + Flatten + Linear + Softmax\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=size, padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * size * size, action_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Value head: Convolution + Batch Normalization + ReLU + Flatten + Linear + Tanh\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, size, kernel_size=size, padding='same'),\n",
    "            nn.BatchNorm2d(size),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(size * size * size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Set the model to run on the selected device\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
