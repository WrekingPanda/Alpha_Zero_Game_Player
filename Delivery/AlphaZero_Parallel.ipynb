{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Zero Parallel\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook contains an AlphaZero implementation capable of running parallel iterations. Additionally, a data augmentation function was developed to increase the size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ataxx import AttaxxBoard\n",
    "from go import GoBoard\n",
    "from aMCTS_parallel import MCTSParallel\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Data augmentation is used to artificially increase the size of the training set by applying transformations to the original data set. In the context of this project, rotations and reflections were applied to the board matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations(board_state, action_probs, outcome, gameType):\n",
    "    # Transformations for the Go Board\n",
    "    if gameType == 'G':\n",
    "        side = board_state.size\n",
    "        transf = []\n",
    "        transf.append((board_state.flip_vertical().EncodedGameStateChanged(), np.append(np.flip(np.copy(action_probs)[:-1].reshape(side,side),0).flatten(),action_probs[-1]), outcome))                         # flip vertically\n",
    "        transf.append((board_state.rotate90(1).EncodedGameStateChanged(), np.append(np.rot90(np.copy(action_probs)[:-1].reshape(side,side),1).flatten(),action_probs[-1]), outcome))                            # rotate 90\n",
    "        transf.append((board_state.rotate90(1).flip_vertical().EncodedGameStateChanged(), np.append(np.rot90(np.flip(np.copy(action_probs)[:-1].reshape(side,side),1),0).flatten(),action_probs[-1]), outcome)) # rotate 90 and flip vertically\n",
    "        transf.append((board_state.rotate90(2).EncodedGameStateChanged(), np.append(np.rot90(np.copy(action_probs)[:-1].reshape(side,side),2).flatten(),action_probs[-1]), outcome))                            # rotate 180\n",
    "        transf.append((board_state.rotate90(2).flip_vertical().EncodedGameStateChanged(), np.append(np.rot90(np.flip(np.copy(action_probs)[:-1].reshape(side,side),1),0).flatten(),action_probs[-1]), outcome)) # rotate 180 and flip vertically\n",
    "        transf.append((board_state.rotate90(3).EncodedGameStateChanged(), np.append(np.rot90(np.copy(action_probs)[:-1].reshape(side,side),3).flatten(),action_probs[-1]), outcome))                            # rotate 270\n",
    "        transf.append((board_state.rotate90(3).flip_vertical().EncodedGameStateChanged(), np.append(np.rot90(np.flip(np.copy(action_probs)[:-1].reshape(side,side),1),0).flatten(),action_probs[-1]), outcome)) # rotate 270 and flip vertically\n",
    "        return transf\n",
    "    \n",
    "    # Transformations for the Ataxx Board\n",
    "    elif gameType == 'A':\n",
    "        side = board_state.size\n",
    "        transf = []\n",
    "        transf.append((board_state.flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.copy(action_probs).reshape(side,side,side,side),2),0).flatten(), outcome))                                                 # flip vertically\n",
    "        transf.append((board_state.rotate90(1).EncodedGameStateChanged(), np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),1,(2,3)),1,(0,1)).flatten(), outcome))                                       # rotate 90\n",
    "        transf.append((board_state.rotate90(1).flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),1,(2,3)),1,(0,1)),2),0).flatten(), outcome)) # rotate 90 and flip vertically\n",
    "        transf.append((board_state.rotate90(2).EncodedGameStateChanged(), np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),2,(2,3)),2,(0,1)).flatten(), outcome))                                       # rotate 180\n",
    "        transf.append((board_state.rotate90(2).flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),2,(2,3)),2,(0,1)),2),0).flatten(), outcome)) # rotate 180 and flip vertically\n",
    "        transf.append((board_state.rotate90(3).EncodedGameStateChanged(), np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),3,(2,3)),3,(0,1)).flatten(), outcome))                                       # rotate 270\n",
    "        transf.append((board_state.rotate90(3).flip_vertical().EncodedGameStateChanged(), np.flip(np.flip(np.rot90(np.rot90(np.copy(action_probs).reshape(side,side,side,side),3,(2,3)),3,(0,1)),2),0).flatten(), outcome)) # rotate 270 and flip vertically\n",
    "        return transf\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Zero Parallel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel2:\n",
    "    def __init__(self, model, optimizer, board, gameType, data_augmentation=False, verbose=False, **params):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.board = board\n",
    "        self.gameType = gameType\n",
    "        self.params = params\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    # SELF PLAY PHASE -------------------------------------------------------------------------------------\n",
    "    def SelfPlay(self):\n",
    "        return_dataset = []\n",
    "        boards = [None for _ in range(self.params[\"n_self_play_parallel\"])]\n",
    "        boards_dataset = [[] for _ in range(self.params[\"n_self_play_parallel\"])]\n",
    "        \n",
    "        for i in range(self.params[\"n_self_play_parallel\"]):\n",
    "            boards[i] = AttaxxBoard(self.board.size) if self.gameType == \"A\" else GoBoard(self.board.size)\n",
    "            boards[i].Start(render=False)\n",
    "\n",
    "        self.mcts = MCTSParallel(self.params[\"mcts_iterations\"], self.model)\n",
    "\n",
    "        while len(boards) > 0:\n",
    "            boards_actions_probs = self.mcts.Search(boards)\n",
    "\n",
    "            for i in range(len(boards))[::-1]:\n",
    "                action_probs = boards_actions_probs[i]\n",
    "                boards_dataset[i].append((deepcopy(boards[i]), action_probs, boards[i].player))\n",
    "                moves = list(range(len(action_probs)))\n",
    "                action = np.random.choice(moves, p=action_probs)\n",
    "                move = self.mcts.roots[i].children[action].originMove\n",
    "                boards[i].Move(move)\n",
    "                boards[i].NextPlayer()\n",
    "                boards[i].CheckFinish()\n",
    "\n",
    "                if boards[i].hasFinished():\n",
    "                    boards_dataset[i].append((deepcopy(boards[i]), action_probs, boards[i].player)) # add the final config\n",
    "                    for board, action_probs, player in boards_dataset[i]:\n",
    "                        outcome = 1 if player==board.winner else -1\n",
    "                        return_dataset.append((board.EncodedGameStateChanged(), action_probs, outcome))\n",
    "\n",
    "                        # Data augmentation process (rotating and flipping the board)\n",
    "                        if self.data_augmentation:\n",
    "                            for transformed_data in transformations(board, action_probs, outcome, self.gameType):\n",
    "                                return_dataset.append(transformed_data)\n",
    "\n",
    "                    del boards[i]\n",
    "\n",
    "        return return_dataset\n",
    "\n",
    "    \n",
    "    # TRAINING MODEL ----------------------------------------------------------------------------------------\n",
    "    def Train(self, dataset):\n",
    "        random.shuffle(dataset)\n",
    "        for batch_index in range(0, len(dataset), self.params['batch_size']):\n",
    "            sample = dataset[batch_index : batch_index+self.params[\"batch_size\"]]\n",
    "            board_encoded, policy_targets, value_targets = zip(*sample)\n",
    "            board_encoded, policy_targets, value_targets = np.array(board_encoded), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            board_encoded = torch.tensor(board_encoded, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(board_encoded)\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    # LEARN MODEL ------------------------------------------------------------------------------------------\n",
    "    def Learn(self):\n",
    "        for iteration in tqdm(range(self.params[\"n_iterations\"]), desc=\"AlphaZero Algorithm Iterations\", leave=False, unit=\"iter\", ncols=100, colour=\"#fc6a65\"):\n",
    "            dataset = []\n",
    "\n",
    "            self.model.eval()\n",
    "            for sp_iteration in tqdm(range(self.params[\"self_play_iterations\"]//self.params[\"n_self_play_parallel\"]), desc=\"Self-Play Iterations\", leave=False, unit=\"iter\", ncols=100, colour=\"#fca965\"):\n",
    "                dataset += self.SelfPlay()\n",
    "            \n",
    "            self.model.train()\n",
    "            for epoch in tqdm(range(self.params[\"n_epochs\"]), desc=\"Training Model\", leave=False, unit=\"epoch\", ncols=100, colour=\"#9ffc65\"):\n",
    "                self.Train(dataset)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"./Models/{str.upper(self.gameType)}{self.board.size}/{str.upper(self.gameType)}{self.board.size}_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"./Optimizers/{str.upper(self.gameType)}{self.board.size}/{str.upper(self.gameType)}{self.board.size}_{iteration}_opt.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
