{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha MCTS Parallel\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook contains a Monte Carlo Tree Search implementation capabel of running parallel iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ataxx import AttaxxBoard\n",
    "from go import GoBoard\n",
    "\n",
    "import numpy \n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Node:\n",
    "    def __init__(self, board, parent=None, move=None, policy_value=0) -> None:\n",
    "        self.board = board\n",
    "        self.w = 0 # Sum of backpropagation \n",
    "        self.n = 0 # Num of visits\n",
    "        self.p = policy_value # Probability returned from NN\n",
    "        \n",
    "        self.originMove = move # Save move that origined self \n",
    "        self.parent = parent # Save parent node\n",
    "        self.children = {} # Save all children \n",
    "\n",
    "\n",
    "    # SELECTION PHASE ---------------------------------------------------------\n",
    "    def Select(self):\n",
    "        c = 2\n",
    "        max_ucb = -numpy.inf\n",
    "        best_node = []\n",
    "\n",
    "        # Check if there are no children to search from\n",
    "        if len(self.children) == 0:\n",
    "            return self\n",
    "        \n",
    "        # Choose best children \n",
    "        for child in self.children.values():\n",
    "            # Calculate UCB for each children\n",
    "            ucb = child.w/child.n + child.p*c*(self.n**(1/2))/(1+child.n)  if child.n != 0 else child.p*c*(self.n**(1/2))/(1+child.n)\n",
    "\n",
    "            # Update max UCB value, as well as best Node\n",
    "            if ucb > max_ucb: \n",
    "                max_ucb = ucb\n",
    "                best_node = [child]\n",
    "            elif ucb == max_ucb:\n",
    "                best_node.append(child)\n",
    "\n",
    "        return numpy.random.choice(best_node)\n",
    "\n",
    "\n",
    "    # EXPANSION PHASE ---------------------------------------------------------\n",
    "    def Expansion(self, policy):\n",
    "        # Check if there are no children or the game is over\n",
    "        if len(self.children) != 0 or self.board.winner != 0:\n",
    "            return\n",
    "        \n",
    "        # List of all possible moves\n",
    "        possibleMoves = self.board.PossibleMoves() \n",
    "\n",
    "        # Iterate through all possible moves \n",
    "        for move in possibleMoves:\n",
    "            # Create a copy of original board\n",
    "            cur_board = deepcopy(self.board)\n",
    "            # Play the choosen move\n",
    "            cur_board.Move(move)\n",
    "            action = cur_board.MoveToAction(move)\n",
    "            # Change player\n",
    "            cur_board.NextPlayer()\n",
    "            # Check is the game is over\n",
    "            cur_board.CheckFinish()\n",
    "\n",
    "            # Update children \n",
    "            self.children[action] = MCTS_Node(cur_board, self, move, policy_value=policy[action])\n",
    "\n",
    "\n",
    "    # BACKPROPAGATION PHASE ----------------------------------------------------\n",
    "    def BackPropagation(self, value):\n",
    "        # Update W and N values\n",
    "        self.w += value\n",
    "        self.n += 1\n",
    "\n",
    "        # If current state has a parent, backpropagate value\n",
    "        if self.parent is not None:\n",
    "            self.parent.BackPropagation(-1*value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, n_iterations, model, dirichlet_eps=0.25) -> None:\n",
    "        self.n_iterations = n_iterations # Num. of iterations for MCTS\n",
    "        self.model = model # completeee\n",
    "        self.dirichlet_eps = dirichlet_eps # completee\n",
    "        self.roots = [] # Completeee\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "\n",
    "\n",
    "    # SEARCH PHASE -----------------------------------------------------------\n",
    "    def Search(self, boards_obj_list):\n",
    "        self.roots = [MCTS_Node(board) for board in boards_obj_list]\n",
    "\n",
    "        # Add noise to the roots' policy array\n",
    "        boards_states = [root.board.EncodedGameStateChanged() for root in self.roots]\n",
    "        boards_states = torch.tensor(boards_states, device=self.model.device)\n",
    "\n",
    "        policy, _ = self.model(boards_states)\n",
    "        policy = policy.cpu().numpy()\n",
    "\n",
    "        action_space_size = self.roots[0].board.size**4 if type(self.roots[0].board)==AttaxxBoard else self.roots[0].board.size**2+1\n",
    "        dirichlet_arr = numpy.ones(shape=action_space_size)/action_space_size\n",
    "\n",
    "        policy = (1-self.dirichlet_eps)*policy + self.dirichlet_eps*numpy.random.dirichlet(dirichlet_arr)\n",
    "        \n",
    "        \n",
    "        # Expand each root and associate the policy valyes with the ramifications\n",
    "        for i in range(len(boards_obj_list)):\n",
    "            self.roots[i].Expansion(policy[i])\n",
    "            self.roots[i].n = 1\n",
    "\n",
    "\n",
    "        # Start MCTS iterations\n",
    "        for _ in tqdm(range(self.n_iterations), desc=\"MCTS Iterations\", leave=False, unit=\"iter\", ncols=100, colour=\"#f7fc65\"):             \n",
    "            nodes = [self.roots[i].Select() for i in range(len(boards_obj_list))]\n",
    "\n",
    "            # Selection phase\n",
    "            for i in range(len(boards_obj_list)):\n",
    "                while len(nodes[i].children) > 0:\n",
    "                    nodes[i] = nodes[i].Select() \n",
    "\n",
    "            # Get the values from NN\n",
    "            boards_states = [nodes[i].board.EncodedGameStateChanged() for i in range(len(boards_obj_list))]\n",
    "            boards_states = torch.tensor(boards_states, device=self.model.device)\n",
    "            policy, value = self.model(boards_states)\n",
    "            policy = policy.cpu().numpy()\n",
    "            value = value.cpu().numpy()\n",
    "\n",
    "            for i in range(len(boards_obj_list)):\n",
    "                # Expansion phase\n",
    "                nodes[i].Expansion(policy[i])\n",
    "\n",
    "                # Backpropagation phase\n",
    "                nodes[i].BackPropagation(value[i][0])\n",
    "        \n",
    "\n",
    "        # Return the actions' probabilities for each game\n",
    "        action_space_size = self.roots[0].board.size**4 if type(self.roots[0].board) == AttaxxBoard else (self.roots[0].board.size**2)+1\n",
    "        boards_actions_probs = [numpy.zeros(shape=action_space_size) for _ in range(len(boards_obj_list))]\n",
    "        \n",
    "        for i in range(len(boards_obj_list)):\n",
    "            for action, child in self.roots[i].children.items():\n",
    "                boards_actions_probs[i][action] = child.n\n",
    "            boards_actions_probs[i] /= numpy.sum(boards_actions_probs[i])\n",
    "            \n",
    "        return boards_actions_probs\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
