{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MMQq4ADizWHE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zM1qzD05xtcG"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, padding=1)\n",
        "\n",
        "def conv1x1(in_planes, out_planes):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, padding=0)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, board_size, action_size, num_resBlocks=20, num_hidden=128):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initial convolution\n",
        "        self.startBlock = nn.Sequential(\n",
        "            conv3x3(3, num_hidden),\n",
        "            nn.BatchNorm2d(num_hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Loop of all 20 Residual Layers\n",
        "        self.backBone = nn.ModuleList(\n",
        "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
        "        )\n",
        "\n",
        "\n",
        "        # Outputs expected value of the state\n",
        "        self.valueHead = nn.Sequential(\n",
        "            conv1x1(num_hidden, 1),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "\n",
        "            nn.Linear(in_features=board_size**2, out_features=num_hidden),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(in_features=num_hidden, out_features=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "        # Outputs the probabilities of each possible action\n",
        "        self.policyHead = nn.Sequential(\n",
        "            conv1x1(num_hidden, 2),\n",
        "            nn.BatchNorm2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2*board_size**2, out_features=(action_size)),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.startBlock(x)\n",
        "\n",
        "        for resBlock in self.backBone:\n",
        "            x = resBlock(x)\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        \n",
        "        policy = self.policyHead(x)\n",
        "        value = self.valueHead(x)\n",
        "\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_hidden):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(num_hidden, num_hidden)\n",
        "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "        self.conv2 = conv3x3(num_hidden, num_hidden)\n",
        "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Skip connections\n",
        "        out = self.relu(out + identity)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "5GTysHwnzcVq",
        "outputId": "34775de7-f3bc-4ae7-d7c6-0a77083ac3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 128, 10, 10]           3,584\n",
            "       BatchNorm2d-2          [-1, 128, 10, 10]             256\n",
            "              ReLU-3          [-1, 128, 10, 10]               0\n",
            "            Conv2d-4          [-1, 128, 10, 10]         147,584\n",
            "       BatchNorm2d-5          [-1, 128, 10, 10]             256\n",
            "              ReLU-6          [-1, 128, 10, 10]               0\n",
            "            Conv2d-7          [-1, 128, 10, 10]         147,584\n",
            "       BatchNorm2d-8          [-1, 128, 10, 10]             256\n",
            "              ReLU-9          [-1, 128, 10, 10]               0\n",
            "         ResBlock-10          [-1, 128, 10, 10]               0\n",
            "           Conv2d-11          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 10, 10]             256\n",
            "             ReLU-13          [-1, 128, 10, 10]               0\n",
            "           Conv2d-14          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-15          [-1, 128, 10, 10]             256\n",
            "             ReLU-16          [-1, 128, 10, 10]               0\n",
            "         ResBlock-17          [-1, 128, 10, 10]               0\n",
            "           Conv2d-18          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-19          [-1, 128, 10, 10]             256\n",
            "             ReLU-20          [-1, 128, 10, 10]               0\n",
            "           Conv2d-21          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-22          [-1, 128, 10, 10]             256\n",
            "             ReLU-23          [-1, 128, 10, 10]               0\n",
            "         ResBlock-24          [-1, 128, 10, 10]               0\n",
            "           Conv2d-25          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-26          [-1, 128, 10, 10]             256\n",
            "             ReLU-27          [-1, 128, 10, 10]               0\n",
            "           Conv2d-28          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-29          [-1, 128, 10, 10]             256\n",
            "             ReLU-30          [-1, 128, 10, 10]               0\n",
            "         ResBlock-31          [-1, 128, 10, 10]               0\n",
            "           Conv2d-32          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-33          [-1, 128, 10, 10]             256\n",
            "             ReLU-34          [-1, 128, 10, 10]               0\n",
            "           Conv2d-35          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-36          [-1, 128, 10, 10]             256\n",
            "             ReLU-37          [-1, 128, 10, 10]               0\n",
            "         ResBlock-38          [-1, 128, 10, 10]               0\n",
            "           Conv2d-39          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-40          [-1, 128, 10, 10]             256\n",
            "             ReLU-41          [-1, 128, 10, 10]               0\n",
            "           Conv2d-42          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-43          [-1, 128, 10, 10]             256\n",
            "             ReLU-44          [-1, 128, 10, 10]               0\n",
            "         ResBlock-45          [-1, 128, 10, 10]               0\n",
            "           Conv2d-46          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-47          [-1, 128, 10, 10]             256\n",
            "             ReLU-48          [-1, 128, 10, 10]               0\n",
            "           Conv2d-49          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-50          [-1, 128, 10, 10]             256\n",
            "             ReLU-51          [-1, 128, 10, 10]               0\n",
            "         ResBlock-52          [-1, 128, 10, 10]               0\n",
            "           Conv2d-53          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-54          [-1, 128, 10, 10]             256\n",
            "             ReLU-55          [-1, 128, 10, 10]               0\n",
            "           Conv2d-56          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-57          [-1, 128, 10, 10]             256\n",
            "             ReLU-58          [-1, 128, 10, 10]               0\n",
            "         ResBlock-59          [-1, 128, 10, 10]               0\n",
            "           Conv2d-60          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-61          [-1, 128, 10, 10]             256\n",
            "             ReLU-62          [-1, 128, 10, 10]               0\n",
            "           Conv2d-63          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-64          [-1, 128, 10, 10]             256\n",
            "             ReLU-65          [-1, 128, 10, 10]               0\n",
            "         ResBlock-66          [-1, 128, 10, 10]               0\n",
            "           Conv2d-67          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-68          [-1, 128, 10, 10]             256\n",
            "             ReLU-69          [-1, 128, 10, 10]               0\n",
            "           Conv2d-70          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-71          [-1, 128, 10, 10]             256\n",
            "             ReLU-72          [-1, 128, 10, 10]               0\n",
            "         ResBlock-73          [-1, 128, 10, 10]               0\n",
            "           Conv2d-74          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-75          [-1, 128, 10, 10]             256\n",
            "             ReLU-76          [-1, 128, 10, 10]               0\n",
            "           Conv2d-77          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-78          [-1, 128, 10, 10]             256\n",
            "             ReLU-79          [-1, 128, 10, 10]               0\n",
            "         ResBlock-80          [-1, 128, 10, 10]               0\n",
            "           Conv2d-81          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-82          [-1, 128, 10, 10]             256\n",
            "             ReLU-83          [-1, 128, 10, 10]               0\n",
            "           Conv2d-84          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-85          [-1, 128, 10, 10]             256\n",
            "             ReLU-86          [-1, 128, 10, 10]               0\n",
            "         ResBlock-87          [-1, 128, 10, 10]               0\n",
            "           Conv2d-88          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-89          [-1, 128, 10, 10]             256\n",
            "             ReLU-90          [-1, 128, 10, 10]               0\n",
            "           Conv2d-91          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-92          [-1, 128, 10, 10]             256\n",
            "             ReLU-93          [-1, 128, 10, 10]               0\n",
            "         ResBlock-94          [-1, 128, 10, 10]               0\n",
            "           Conv2d-95          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-96          [-1, 128, 10, 10]             256\n",
            "             ReLU-97          [-1, 128, 10, 10]               0\n",
            "           Conv2d-98          [-1, 128, 10, 10]         147,584\n",
            "      BatchNorm2d-99          [-1, 128, 10, 10]             256\n",
            "            ReLU-100          [-1, 128, 10, 10]               0\n",
            "        ResBlock-101          [-1, 128, 10, 10]               0\n",
            "          Conv2d-102          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-103          [-1, 128, 10, 10]             256\n",
            "            ReLU-104          [-1, 128, 10, 10]               0\n",
            "          Conv2d-105          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-106          [-1, 128, 10, 10]             256\n",
            "            ReLU-107          [-1, 128, 10, 10]               0\n",
            "        ResBlock-108          [-1, 128, 10, 10]               0\n",
            "          Conv2d-109          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-110          [-1, 128, 10, 10]             256\n",
            "            ReLU-111          [-1, 128, 10, 10]               0\n",
            "          Conv2d-112          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-113          [-1, 128, 10, 10]             256\n",
            "            ReLU-114          [-1, 128, 10, 10]               0\n",
            "        ResBlock-115          [-1, 128, 10, 10]               0\n",
            "          Conv2d-116          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-117          [-1, 128, 10, 10]             256\n",
            "            ReLU-118          [-1, 128, 10, 10]               0\n",
            "          Conv2d-119          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-120          [-1, 128, 10, 10]             256\n",
            "            ReLU-121          [-1, 128, 10, 10]               0\n",
            "        ResBlock-122          [-1, 128, 10, 10]               0\n",
            "          Conv2d-123          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-124          [-1, 128, 10, 10]             256\n",
            "            ReLU-125          [-1, 128, 10, 10]               0\n",
            "          Conv2d-126          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-127          [-1, 128, 10, 10]             256\n",
            "            ReLU-128          [-1, 128, 10, 10]               0\n",
            "        ResBlock-129          [-1, 128, 10, 10]               0\n",
            "          Conv2d-130          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-131          [-1, 128, 10, 10]             256\n",
            "            ReLU-132          [-1, 128, 10, 10]               0\n",
            "          Conv2d-133          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-134          [-1, 128, 10, 10]             256\n",
            "            ReLU-135          [-1, 128, 10, 10]               0\n",
            "        ResBlock-136          [-1, 128, 10, 10]               0\n",
            "          Conv2d-137          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-138          [-1, 128, 10, 10]             256\n",
            "            ReLU-139          [-1, 128, 10, 10]               0\n",
            "          Conv2d-140          [-1, 128, 10, 10]         147,584\n",
            "     BatchNorm2d-141          [-1, 128, 10, 10]             256\n",
            "            ReLU-142          [-1, 128, 10, 10]               0\n",
            "        ResBlock-143          [-1, 128, 10, 10]               0\n",
            "          Conv2d-144            [-1, 2, 10, 10]             258\n",
            "     BatchNorm2d-145            [-1, 2, 10, 10]               4\n",
            "            ReLU-146            [-1, 2, 10, 10]               0\n",
            "         Flatten-147                  [-1, 200]               0\n",
            "          Linear-148                  [-1, 101]          20,301\n",
            "         Softmax-149                  [-1, 101]               0\n",
            "          Conv2d-150            [-1, 1, 10, 10]             129\n",
            "     BatchNorm2d-151            [-1, 1, 10, 10]               2\n",
            "            ReLU-152            [-1, 1, 10, 10]               0\n",
            "         Flatten-153                  [-1, 100]               0\n",
            "          Linear-154                  [-1, 128]          12,928\n",
            "            ReLU-155                  [-1, 128]               0\n",
            "          Linear-156                    [-1, 1]             129\n",
            "            Tanh-157                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 5,951,191\n",
            "Trainable params: 5,951,191\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 13.98\n",
            "Params size (MB): 22.70\n",
            "Estimated Total Size (MB): 36.68\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Net(10, 10**2+1)\n",
        "\n",
        "print(summary(model,(3,10,10)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
